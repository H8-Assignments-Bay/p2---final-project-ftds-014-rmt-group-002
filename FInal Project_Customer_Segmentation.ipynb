{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagian ini berisi deskripsi dari project dan library yang digunakan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1. Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Handoko Pramulyo | Batch : FTDS-014\n",
    "\n",
    "Title of the Project : **Elevating sales performance through predictive customer segmentation**\n",
    "\n",
    "From a marketing perspective, it makes perfect sense to put in the effort to understand the characteristics and preferences of your best customers and keep sustaining your business. In this project, we predict customer segmentation and its actionable insight to push your marketing personalized programs at their best for the customer and boost your sales performance.\n",
    "\n",
    "**Data Description** : Data yang digunakan adalah data sales retail online suatu perusahaan international yang berbasis di UK Inggris (A real online retail transaction data set of two years) dengan beragam produk hadiah unik, rumahan, furniture, tools, dan masih banyak lagi. Dataset terdiri dari beberapa kolom, seperti nomor invoice, tanggal invoice (range in 01/12/2009 sd 09/12/2011), nama barang, jumlah barang, harga barang, nama customer, serta lokasi customer. Adapun kebanyakan dari customer toko ini adalah glosir. Data-data ini akan kita proses lebih lanjut melalui RFM analysis dengan segmentasi-nya menggunakan model KMeans.\n",
    "\n",
    "**Objective** : `Segmentasi customer in UK dan action promo apa yang sebaiknya dilakukan based on RFM`\n",
    "\n",
    "**Dataset link** : https://www.kaggle.com/datasets/mashlyn/online-retail-ii-uci?select=online_retail_II.csv\n",
    "\n",
    "**Deployment link** : https://customer-segmentation-uk.herokuapp.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgements ----------------------**\n",
    "\n",
    "- Chen, D. Sain, S.L., and Guo, K. (2012), Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208.\n",
    "- Chen, D., Guo, K. and Ubakanma, G. (2015), Predicting customer profitability over time based on RFM time series, International Journal of Business Forecasting and Marketing Intelligence, Vol. 2, No. 1, pp.1-18.\n",
    "- Chen, D., Guo, K., and Li, Bo (2019), Predicting Customer Profitability Dynamically over Time: An Experimental Comparative Study, 24th Iberoamerican Congress on Pattern Recognition (CIARP 2019), Havana, Cuba, 28-31 Oct, 2019.\n",
    "- Laha Ale, Ning Zhang, Huici Wu, Dajiang Chen, and Tao Han, Online Proactive Caching in Mobile Edge Computing Using Bidirectional Deep Recurrent Neural Network, IEEE Internet of Things Journal, Vol. 6, Issue 3, pp. 5520-5530, 2019.\n",
    "- Rina Singh, Jeffrey A. Graves, Douglas A. Talbert, William Eberle, Prefix and Suffix Sequential Pattern Mining, Industrial Conference on Data Mining 2018: Advances in Data Mining. Applications and Theoretical Aspects, pp. 309-324. 2018.\n",
    "\n",
    "**-----------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import style\n",
    "import squarify\n",
    "import warnings\n",
    "import datetime as dt\n",
    "from scipy import stats\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "\n",
    "# Define plot style\n",
    "plt.style.use('bmh') #('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada case Machine Learning ini, new library yang akan digunakan adalah scikit-learn (Machine Learning library in Python), library yang dedicated dibuat untuk Machine Learning. Simple and efficient tools for predictive data analysis, Accessible to everybody, and reusable in various contexts, Built on NumPy, SciPy, and matplotlib, Open source, dan sekaligus commercially usable. OK, lets proceed to the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Data Loading & Converting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagian berisi proses loading data, cek valid atau tidaknya data yang akan digunakan, serta konversi dataset dalam format RFM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada bagian ini, kita akan coba isi dengan proses penyiapan data (pd read) sebelum dilakukan eksplorasi data lebih lanjut (atau yang biasa disebut dengan EDA). Proses Data Loading dapat berupa memberi nama baru untuk setiap kolom (opsional), melihat gambaran umum dari data, mengecek ukuran dataset, dll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset \n",
    "df = pd.read_csv('online_retail.csv') # Read CSV file\n",
    "# data = pd.read_excel('online_retail_simple.xlsx', index_col=None, header=None) # # Read Excel file\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show shape info of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data observasi OKE (lebih dari 50 jumlah observasi --> based on sclearn recommendation) dengan jumlah feature yang proporsional, maka yang bisa kita artikan bahwa model yang akan dibuat 'most likely' low possibility untuk overfit (due to low variance which is good. ```Additional conclusion based on least square``` Jennifer Zhao in medium.com). **So now we can learn that we have TON of data (MS Excel cant handle TON of data like this).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate Dataset\n",
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Dataset (looking up for missing value and understanding the dataset Dtype)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data type terdiri dari object, int64, dan float64. Terdapat null data pada `Description` dan `Customer ID`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Data validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Data Check (from invalid price, quantity, nulled data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see deeper with the data! Untuk beberapa case dataset, sering kali ditemukan nilai value yang tidak semestinya digunakan pada analisis. Seperti nilai frekuensi sales yang minus, atau harga product yang minus, dll. Demikian, pengecekan value tersebut perlu dilakukan --> drop value tersebut untuk clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Price blow 0\n",
    "data.loc[(data.Price<0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ternyata terdapat beberapa data Price dengan nilai negative! Price dengan value negative sangat tidak common. Row ini perlu kita drop karena nilai price tersebut hanya adjustment accounting dari toko online (seperti Adjust bad debt), bukan real sales (do not represent actual sales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Quantity blow 0\n",
    "data.loc[(data.Quantity<0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nilai Quantity negative pada dasarnya juga tidak common, kecuali jika angka tersebut merepresentasikan product return atau cancelled by customer --> **Lets drop the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show nulled data in Description column\n",
    "data[pd.isnull(data['Description'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terdapat nama product, atau Description, dengan value NaN --> **Lets drop the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show nulled data in Customer ID column\n",
    "data[pd.isnull(data['Customer ID'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terdapat Customer ID NaN --> **Lets drop the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Drop invalid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of data raw\n",
    "print('Shape before :', data.shape)\n",
    "\n",
    "# Remove negative value of Price\n",
    "data = data[data.Price > 0]\n",
    "# Remove negative value of Quantity\n",
    "data = data[data.Quantity > 0]\n",
    "# Remove nulled data in Description column\n",
    "data = data[pd.notnull(data['Description'])]\n",
    "# Remove nulled data in CustomerID column\n",
    "data = data[pd.notnull(data['Customer ID'])]\n",
    "\n",
    "# Print shape of data after cleaning process\n",
    "print('Shape after :', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sum of dropped data :', (100 - (805549/1067371)*100), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck nulled data in dataset\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all data has been cleaned up! No more Nulled or NaN data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Define Dataset Location & Add Total Price Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show value frequency of Country\n",
    "data_country = pd.DataFrame(data.Country.value_counts())\n",
    "# Show top 10 frequency\n",
    "data_country = data_country.head(10)\n",
    "data_country.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari data tersebut, dapat terlihat bahwa jumlah data dari United Kingdom adalah yang terbanyak. Mengingat segmentasi customer sangat dipengaruhi oleh lokasi (geografi), maka pada case ini, dataset UK atau Inggris saja yang akan digunakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new dataset only for UK\n",
    "data_UK = data[data.Country == 'United Kingdom']\n",
    "\n",
    "# Create new columns of Total price\n",
    "warnings.filterwarnings(\"ignore\") # Ignore warning\n",
    "data_UK['TotalPrice'] = data_UK['Quantity']*data_UK['Price']\n",
    "data_UK.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Valid Dataset UK only ready!** --> `data_UK` will be used for EDA of Original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3. Converting dataset into RFM table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date scheme (cut of date!)\n",
    "NOW = dt.date(2011,12,9)\n",
    "\n",
    "# Create new colums for new date format\n",
    "data_UK['date'] = pd.DatetimeIndex(data_UK.InvoiceDate).date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Recency columns (define as Last_Purchase_Date) groupby Customer ID\n",
    "data_UK_recency = data_UK.groupby(['Customer ID'], as_index=False)['date'].max()\n",
    "data_UK_recency.columns = ['Customer ID','Last_Purchase_Date']\n",
    "\n",
    "# Conversion Last_Purchase_Date columns into recency\n",
    "data_UK_recency['Recency'] = data_UK_recency.Last_Purchase_Date.apply(lambda x:(NOW - x).days)\n",
    "data_UK_recency.drop(columns=['Last_Purchase_Date'],inplace=True)\n",
    "\n",
    "# Create Frequency and Magnitude Columns\n",
    "data_UK_freq_mag = data_UK.groupby('Customer ID').agg({'Invoice'   : lambda x:len(x),\n",
    "                                               'TotalPrice': lambda x:x.sum()})\n",
    "data_UK_freq_mag.rename(columns = {'Invoice' :'Frequency', 'TotalPrice':'Monetary'}, inplace= True)\n",
    "\n",
    "# Merge Recency, Frequency, and Magnitude Columns\n",
    "RFM_UK = data_UK_recency.merge(data_UK_freq_mag, left_on='Customer ID', right_on='Customer ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking up dataframe of RFM UK\n",
    "RFM_UK.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RFM dataset is created!** --> `RFM_UK` will be used for EDA of RFM dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagian ini berisi penjelasan dari EDA dataset original (`data_UK`) dan dataset dengan format RFM (`RFM_UK`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1. EDA of Original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see deeper about our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data_UK dataset\n",
    "data_UK.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show value frequency of Invoice, top x frequency\n",
    "data_Invoice = pd.DataFrame(data.Invoice.value_counts()).head(20)\n",
    "# Show value frequency of Invoice, top x frequency\n",
    "data_Description = pd.DataFrame(data.Description.value_counts()).head(3)\n",
    "# Show value frequency of Invoice, top x frequency\n",
    "data_StockCode = pd.DataFrame(data.StockCode.value_counts()).head(15)\n",
    "# Show value frequency of Invoice, top x frequency\n",
    "data_CustomerID = pd.DataFrame(data['Customer ID'].value_counts()).head(10)\n",
    "# Show value Total Price vs date\n",
    "data_date_TotalPrice = pd.DataFrame(data_UK.groupby('date')['TotalPrice'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot chart\n",
    "fig, ax = plt.subplots(1,3, figsize=(20,5))\n",
    "\n",
    "data_Invoice.plot(kind='line', ax=ax[0])\n",
    "ax[0].set_title('Top Invoice (Chart 1)')\n",
    "\n",
    "data_StockCode.plot(kind='area', ax=ax[1])\n",
    "ax[1].set_title('Top StockCode (Chart 2)')\n",
    "\n",
    "data_CustomerID.plot(kind='bar', ax=ax[2])\n",
    "ax[2].set_title('Top CustomerID (Chart 3)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A few syntax for looking up deeper about the dataset (stat desc)\n",
    "# data.loc[data['Invoice'] == '576339'].groupby('InvoiceDate').sum()\n",
    "# data.loc[data['StockCode'] == '85123A'].groupby('Invoice').sum()\n",
    "# data.loc[data['Description'] == 'WHITE HANGING HEART T-LIGHT HOLDER'].groupby('Invoice').sum()\n",
    "# data.loc[data['Customer ID'] == 17841.0]\n",
    "# data.loc[data['Customer ID'] == 17841.0].groupby('InvoiceDate').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On Chart 1**, it can be seen that there was an invoice transaction of 576339, with a very large amount in terms of amount, and this was done by one buyer of course (because the invoice represents a transaction by one buyer on the same day). The customer with this invoice transaction is likely a large wholesaler, although the amount is not large, the types of goods they buy are very diverse (invoice data: 2011-11-14, Quantity: 2175, Price: 3802.3, Cust ID: 7640032).\n",
    "\n",
    "As for **chart 2**, it can be seen that the number of stock with the most purchases is the stock product with numbers 85123A, 85099B (WHITE HANGING HEART T-LIGHT HOLDER, a kind of love-shaped table decoration art merchandise which is very pretty, and REGENCY CAKESTAND 3 TIER or a kind of stacked plate for a unique cake container with a beautiful design, etc.). These products have been purchased for more than 3.5K to 5K in just 2 years (2 years dataset time range), an extraordinary number when compared to the average overall product with a total sales volume of 2K.\n",
    "\n",
    "In **Chart 3**, it can be seen that the customer with ID 17841.0 is the customer with the most transactions compared to all other customers (almost 13K, in terms of the total quantity of all invoices made on different days ~ 211 invoices), or at least they always make product purchases at retail stores once every 3 to 4 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot chart\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,5))\n",
    "\n",
    "data_country.plot(kind='bar', ax=ax[0])\n",
    "ax[0].set_title('Top Country (Chart 4)')\n",
    "\n",
    "data_date_TotalPrice.plot(kind='area', ax=ax[1])\n",
    "ax[1].set_title('Sales by date (Chart 5)')\n",
    "\n",
    "# set the spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously discussed, **Chart 4** shows that this online retail store sells its products almost all over the world, with the majority of its customers domiciled in the UK or England, the second in Germany, followed by EIRE (Ireland), France, and so on. However, it can be seen that the UK GAP with other countries in terms of the number of transactions is very different (UK majority ~70% of sales transactions). Therefore, based on several sources, we chose the UK as a study case because customer segmentation also needs to be divided by geography (source: ocbcnisp.com, Market Segmentation: \"How to Do It and How to Profit from It\" by Malcolm McDonald), and the UK dominates the overall dataset.\n",
    "\n",
    "As for **Chart 5**, it can be seen that sales are not smooth (spike, valley, etc.). However, overall sales cycle patterns can be seen at several moments throughout the dataset year, namely from 2010 to the end of 2011. There is an increase in sales at the end of the year, namely the moment when UK residents celebrate Christmas and New Year which makes buying and selling transactions in the UK move faster and more often than during other months. Sales fell slightly before and after Christmas, or during Q1 and Q3 because almost every country and world organization were in a state of planning and evaluation. UK residents are mostly saving to prepare their savings for the new year and see what the economic conditions are like in Q2, thus sales at this time are down, slightly to a moderate amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2. EDA of RFM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show RFM_UK dataset\n",
    "RFM_UK.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Histogram and Scatter Plot\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1), sns.distplot(RFM_UK['Recency'], bins=20, kde=True), plt.title('Recency Histogram (Chart 6)')\n",
    "plt.subplot(1, 3, 2), sns.distplot(RFM_UK['Frequency'], bins=200, kde=True), plt.title('Frequency Histogram (Chart 7)')\n",
    "plt.subplot(1, 3, 3), sns.distplot(RFM_UK['Monetary'], bins=300, kde=True), plt.title('Monetary Histogram (Chart 8)')\n",
    "\n",
    "# set the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Descibe dataframe of RFM describe\n",
    "# RFM_UK.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charts 6 to 8 are the result of converting raw datasets to RFM tables. It can be seen in **Chart 6** that most of the recency of last purchase customers is in the range of 0 to 100 (or about 1 to 3 months), and from the distribution point of view, the data is very right skewed, which means that the recency value is close to 0 which is much higher than that of the data distribution. with recency > 100. In fact, this condition is a plus and a minus from a retail store perspective. The first, is because the customer is quite recent in buying the product at the store which is good, but the second is that the one who bought the recent product is a new customer (potential but might be a risk at giving a bad rating without proper care).\n",
    "\n",
    "In the next chart, **chart 7**, it can be seen that there are several customers with wholesaler transactions whose character is to buy goods very often, wholesalers, wholesalers, retailers, and the like but with a small amount of money and will sell the goods they buy. buy for resale as an agent or distributor at a better price (detected as outliers in frequency, but not in monetary).\n",
    "\n",
    "As for **chart 8**, it can be seen that there are several customers with a fantastic amount of monetary (~600K with an average monetary of other customers of 2K), even though the distribution of monetary data shows that the majority of customers are more in numbers. 0 to 0.3K only for 2 years of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagian ini berisi proses preparation of RFM dataset, log transformation, dan handling outliers-nya (karena karakter dari dataset tidak terdistribusi normal serta model segmentasi yang akan digunakana adalah K-Means, maka outliers handling perlu dilakukan). **Notes: di mulai pada bagian D ini, semua perhitungan akan menggunakan RFM dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D1. Data Preparation (Main & Inference Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of RFM dataset \n",
    "data_rfm = RFM_UK.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat dataset inference dari dataset master\n",
    "# Angka 10 tuk define jumlah observasi inference\n",
    "# Random_state untuk define formula pemilihan data secara random yet wont change\n",
    "data_rfm_inf = data_rfm.sample(10, random_state=23)\n",
    "# Membuat dataset train dan test (exlude inference)\n",
    "data_rfm = data_rfm.drop(data_rfm_inf.index)\n",
    "# Reset Index --> rekomen untuk dilakukan after berhasil dilakukan split antara data train_test dan inference\n",
    "data_rfm.reset_index(drop=True, inplace=True) \n",
    "data_rfm_inf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data rfm tanpa inf\n",
    "data_rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference dataset is ready!\n",
    "data_rfm_inf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oke, now we have 2 datasets, which are Main & Inference data. Each index has been reset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D2. Feature Selection & Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena Customer ID bukan merupakan value dari observasi segmentasi customer --> Lets drop Customer ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Customer ID column\n",
    "data_rfm_raw = data_rfm.copy()\n",
    "data_rfm = data_rfm.drop(columns=['Customer ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk pengecekan kembali, lets check missing value within RFM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Missing Values in `data_rfm`\n",
    "print('Amount of RFM dataset nulled value : ', data_rfm.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck dataframe (after Cust ID has been droped)\n",
    "data_rfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D3. Perform log transformations into dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat segmentasi atau cluster pada dasarnya adalah bagaimana model ML dapat mencari internal similarities atau pola dari dataset. Similarities ini lah yang akan dijaga selama proses transformasi segmentasi. Mengingat KMeans adalah model yang akan digunakan pada case segmentasi ini, maka beberapa point concern seperti KMeans will not perform if our dataset is skewed, and dataset is not standardized. Demikian, untuk memperkuat similarities pada data dan memperoleh hasil segmentasi yang lebih baik, transformasi log pada dataset bisa dilakukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membuat histogram dengan nilai skewness dan pvalue\n",
    "def check_skew(df_skew, column):\n",
    "    skew = stats.skew(df_skew[column])\n",
    "    skewtest = stats.skewtest(df_skew[column])\n",
    "    plt.title('Distribution of ' + column)\n",
    "    sns.distplot(df_skew[column])\n",
    "    print(\"{}'s: Skew: {}, : {}\".format(column, skew, skewtest))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah chart distribusi dari **Dataset RFM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all 3 graphs together for summary findings\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1), check_skew(data_rfm,'Recency')\n",
    "plt.subplot(1, 3, 2), check_skew(data_rfm,'Frequency')\n",
    "plt.subplot(1, 3, 3), check_skew(data_rfm,'Monetary')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data sangat skewed, oleh karena itu, akan melakukan log transformationuntuk mengurangi kemiringan setiap variabel. Note: small constant is added to make positive value during transformation. Berikut adalah chart distribusi dari **Dataset RFM after log transformation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of data_rfm\n",
    "data_rfm_log = data_rfm.copy()\n",
    "\n",
    "# Transforming dataset data_rfm_log into log\n",
    "data_rfm_log = np.log(data_rfm_log+1)\n",
    "\n",
    "# Plot all 3 graphs together for summary findings\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1), check_skew(data_rfm_log,'Recency')\n",
    "plt.subplot(1, 3, 2), check_skew(data_rfm_log,'Frequency')\n",
    "plt.subplot(1, 3, 3), check_skew(data_rfm_log,'Monetary')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in case we would like to switch scheme without log transformation\n",
    "# data_rfm = data_rfm_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D4. Handling Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Means will be used! Notes : K-means memberikan hasil terbaik dalam kondisi jika distribusi data terbebas dari outliers. Semua feature dari RFM dataset adalah numerical, demikian semua feature akan dilakukan pengecekan sekwness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of column name\n",
    "num_columns = data_rfm.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# # Create chart for loop\n",
    "# plt.figure(figsize=(18, 6))\n",
    "# for i,v in zip(range(len(num_columns)), num_columns):\n",
    "#     plt.subplot(1, 3, i+1), sns.boxplot(data=data_rfm[v]), plt.title(v)\n",
    "\n",
    "# Create chart for loop\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i,v in zip(range(len(num_columns)), num_columns):\n",
    "    plt.subplot(1, 3, i+1), sns.boxplot(data=data_rfm_log[v]), plt.title(v)\n",
    "\n",
    "# set the spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataframe with outliers only and compute the skew\n",
    "pd.DataFrame(data_rfm_log.skew(axis=0), columns=['skewness']).sort_values(by='skewness', ascending=True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teknik outliers handling, windsorizer, dipilih karena semua data outliers ini diasumsikan legitimate (bukan noise yang harus kita drop datanya), demikian kita adjust data outliers ini ke batas atas dan bawahnya sehingga semua informasi dari data tetap dapat diperhitungkan oleh model Machine Learning. Adapun karena dsitribusi data dari semua feature adalah skew, maka formula outliers handling yang digunakan adalah windsorizer IQR fold 3, tail both. Recency: Skew (skewness di antara 0.5 sd 1 atau -0.5 sd -1). Frequency & Monetary : Extrem Skew (skewness > 1 atau < -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the capper | Skewness : skew\n",
    "windsoriser_skew = Winsorizer(capping_method='iqr', tail='both', fold=1.5, # all features skew\n",
    "                          variables=['Recency'])\n",
    "\n",
    "windsoriser_skew.fit(data_rfm_log)\n",
    "data_rfm_log = windsoriser_skew.transform(data_rfm_log)\n",
    "\n",
    "# # Create the capper | Skewness : extreme skew\n",
    "# windsoriser_eskew = Winsorizer(capping_method='iqr', tail='both', fold=1.5, # all features extreme skew\n",
    "#                           variables=['Frequency', 'Monetary'])\n",
    "\n",
    "# windsoriser_eskew.fit(data_rfm_log)\n",
    "# data_rfm_log = windsoriser_eskew.transform(data_rfm_log)\n",
    "\n",
    "# Create the capper | Skewness : normal\n",
    "windsoriser_normal = Winsorizer(capping_method='gaussian', tail='both', fold=2, # all features normal\n",
    "                          variables=['Frequency', 'Monetary'])\n",
    "\n",
    "windsoriser_normal.fit(data_rfm_log)\n",
    "data_rfm_log = windsoriser_normal.transform(data_rfm_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our boxplot after outliers has been handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chart for loop\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i,v in zip(range(len(num_columns)), num_columns):\n",
    "    plt.subplot(1, 3, i+1), sns.boxplot(data=data_rfm_log[v]), plt.title(v)\n",
    "\n",
    "# set the spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outliers has been handled with windsorizer!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. Feature Scaling with Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagian ini berisi proses scaling dataset menggunakan pipeline (plus columns transformer) serta proses fit dan transfromasi dari dataset tersebut yang kemudian akan disebut dengan dataset scaled : `data_rfm_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all columns skewness\n",
    "pd.set_option(\"display.max.columns\",None)\n",
    "pd.DataFrame(data_rfm_log.skew(axis=0), columns=['skewness']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pada case ini, algoritma scaling yang akan digunakan adalah StandardScaler. 1) karena overall feature adalah normal distribution dan 2) memudahkan proses komparasi visual cluster pada saat pembuatan snakeplot.**\n",
    "\n",
    "Note: Berdasarkan masing-masing feature skewness, seharusnya setiap feature dengan skewness = skewed, proses scaling yang digunakan adalah MinMaxScaler (contoh: `Recency`), kemudian untuk feature dengan skewness = normal skew (contoh: `Frequency & Monetary`), maka proses scalingnya menggunakan StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define columnn name for scaling and encoding\n",
    "# numeric_features_mm = ['Recency']\n",
    "# numeric_features_rb = ['Frequency', 'Monetary']\n",
    "numeric_features_ss = ['Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "# # Define scaling and encoding technique\n",
    "# numeric_transformer_mm = Pipeline([(\"scaler1\", MinMaxScaler())])\n",
    "# numeric_transformer_rb = Pipeline([(\"scaler2\", RobustScaler())])\n",
    "numeric_transformer_ss = Pipeline([(\"scaler3\", StandardScaler())])\n",
    "\n",
    "# # Define Transformer Column\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # (\"num1\", numeric_transformer_mm, numeric_features_mm),\n",
    "        (\"num2\", numeric_transformer_ss, numeric_features_ss)\n",
    "        # (\"num3\", numeric_transformer_rb, numeric_features_rb)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "preprocessor.fit(data_rfm_log)\n",
    "# Transform data RFM\n",
    "data_rfm_scaled = preprocessor.transform(data_rfm_log)\n",
    "data_rfm_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F. RMF Segmenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagian ini berisi tentang simulasi jumlah K optimal untuk improvisasi model machine learning unsupervised (into supervised) vs best practice jumlah segmentasi customer versi dari RFM. Setelah dilakukan segmentasi (based on RFM), proses selanjutnya adalah proses conversi RFM value into rank, kemudian dilanjutkan konversi RFM rank into Customer segment (sumber: clevertap.com, learn.g2.com) dengan deskipsi actionable insight untuk setiap customer segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1. Simulate Segmentation amount of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Kmean model with K (trial) 1 sd n\n",
    "k_value = 12  # Define k = 12\n",
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(data_rfm_scaled)\n",
    "                for k in range(1, k_value)]\n",
    "\n",
    "# Measure inersia\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "print('Nilai inersia untuk k 1 sd', k_value, ':', inertias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chart inertia (lihat yg paling patah)\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(range(1, k_value), inertias, \"bo-\")\n",
    "plt.title('Inertia chart (Elbow)')\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.axis([0, 12, 0, 17000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan elbow method, cluster terbaik untuk perfoma model berada pada k = 2 (optimal number of cluster). Namun dalam case RFM ini, jumlah cluster dengan k terbanyak dan proporsional lah yang justru ingin kita identifikasi. Semakin banyak jumlah cluster RFM yang bisa kita temukan, makan semakin tajam segmentasi customer yang bisa kita lakukan (analisi lebih tepat serta insight action yang lebih terarah bagi tim sales dan marketing). Demikian, snake plot akan dilakukan pada bagian selanjutnya!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F2. RMF Segmenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada bagian ini berisi 2 pembahasan. 1) Hasil simulasi visualisasi dari beberapa proses clustering / segmentation untuk scheme k menggunakan scatter plot (dimension has been reduce with PCA into 2 dimension). 2) Hasil simulasi visualisasi yang sama menggunakan snakeplot untuk melihat separasi antar scheme k pada value Recency, Frequency, Monetary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - K Scheme with scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kmeans(normalised_df_rfm, clusters_number, original_df_rfm):\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = clusters_number, random_state = 1)\n",
    "    kmeans.fit(normalised_df_rfm)\n",
    "\n",
    "    # Extract cluster labels\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    # Create a cluster label column in original dataset\n",
    "    df_new = data_rfm.assign(Segmentation = cluster_labels)\n",
    "    \n",
    "    # Transform X_reduce into 2D\n",
    "    model = PCA(n_components=2)\n",
    "    transformed = model.fit_transform(data_rfm_scaled)\n",
    "\n",
    "    # Plot t-SNE\n",
    "    plt.title('Flattened Graph of {} Clusters'.format(clusters_number))\n",
    "    sns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=cluster_labels, style=cluster_labels, palette=\"viridis\")\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 18))\n",
    "\n",
    "plt.subplot(3, 3, 1)\n",
    "data_rfm_k4 = compute_kmeans(data_rfm_scaled, 4, data_rfm)\n",
    "plt.subplot(3, 3, 2)\n",
    "data_rfm_k5 = compute_kmeans(data_rfm_scaled, 5, data_rfm)\n",
    "plt.subplot(3, 3, 3)\n",
    "data_rfm_k6 = compute_kmeans(data_rfm_scaled, 6, data_rfm)\n",
    "plt.subplot(3, 3, 4)\n",
    "data_rfm_k7 = compute_kmeans(data_rfm_scaled, 7, data_rfm)\n",
    "plt.subplot(3, 3, 5)\n",
    "data_rfm_k8 = compute_kmeans(data_rfm_scaled, 8, data_rfm)\n",
    "plt.subplot(3, 3, 6)\n",
    "data_rfm_k9 = compute_kmeans(data_rfm_scaled, 9, data_rfm)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari hasil visualisasi, overall tidak ada data yang overlap antara satu segmentasi dengan segmentasi lainnya (K Means looks just fine for all K scheme, k with the amount of 4 to 9 look fine and clustered nicely). However, masih belum bisa dilihat secara jelas overlap-nya secara amount --> Lets use Snakeplot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - K Scheme with snakeplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snake_plot(normalised_df_rfm, df_rfm_kmeans, df_rfm_original):\n",
    "\n",
    "    normalised_df_rfm = pd.DataFrame(normalised_df_rfm, \n",
    "                                       index=data_rfm.index, \n",
    "                                       columns=data_rfm.columns)\n",
    "    normalised_df_rfm['Segmentation'] = df_rfm_kmeans['Segmentation']\n",
    "\n",
    "    # Melt data into long format\n",
    "    df_melt = pd.melt(normalised_df_rfm.reset_index(), \n",
    "                        id_vars=['Segmentation'],\n",
    "                        value_vars=['Recency', 'Frequency', 'Monetary'], \n",
    "                        var_name='Metric', \n",
    "                        value_name='Value')\n",
    "\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Value')\n",
    "    sns.pointplot(data=df_melt, x='Metric', y='Value', hue='Segmentation')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 18))\n",
    "\n",
    "plt.subplot(3, 3, 1), plt.title('Snake Plot of K-Means = 4')\n",
    "snake_plot(data_rfm_scaled, data_rfm_k4, data_rfm)\n",
    "plt.subplot(3, 3, 2), plt.title('Snake Plot of K-Means = 5')\n",
    "snake_plot(data_rfm_scaled, data_rfm_k5, data_rfm)\n",
    "plt.subplot(3, 3, 3), plt.title('Snake Plot of K-Means = 6')\n",
    "snake_plot(data_rfm_scaled, data_rfm_k6, data_rfm)\n",
    "plt.subplot(3, 3, 4), plt.title('Snake Plot of K-Means = 7')\n",
    "snake_plot(data_rfm_scaled, data_rfm_k7, data_rfm)\n",
    "plt.subplot(3, 3, 5), plt.title('Snake Plot of K-Means = 8')\n",
    "snake_plot(data_rfm_scaled, data_rfm_k8, data_rfm)\n",
    "plt.subplot(3, 3, 6), plt.title('Snake Plot of K-Means = 11')\n",
    "snake_plot(data_rfm_scaled, data_rfm_k9, data_rfm)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan snakeplot, maka dapat disimpukan bahwa jumlah k = 6 adalah yang paling tepat untuk diterapkan dalam RFM analysis karena 1) setiap value dari each k untuk masing-masing Recency, Freq, dan Monetary terlihat tidak bertumpuk terlalu rapat, 2) jumlah k terbanyak yang bisa didapatkan dari keseluruhan scheme of k (the more the cluster we get from RFM, the better the segmentation for sales and marketing, `source: salesforce, United Tractors Tbk, prospectsoft & CleverTap`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Define final Pipeline for Scaling & KMeans model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup file for cintingency\n",
    "data_rfm_optimum = data_rfm_k6.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define final clustering model\n",
    "model_kmeans_pipe = Pipeline([('preprocessing', preprocessor),\n",
    "                            (\"classifier\", KMeans(n_clusters = 6, random_state = 1))])\n",
    "model_kmeans_pipe.fit(data_rfm_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Pipeline `model_kmeans_pipe` untuk proses deployment is ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F3. Define RFM Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group dataframe by segmentation\n",
    "\n",
    " # =======  !!! The key for RFM ranking = Mean/Median show the same result of segmentation\n",
    "data_rfm_optimum_groupby_median = data_rfm_optimum.groupby('Segmentation').median() # Preference Skewed data --> Median\n",
    " # =======  !!! The key for RFM ranking = Mean/Median show the same result of segmentation\n",
    "\n",
    "# Create new column of segmentation freq\n",
    "data_rfm_optimum_groupby_freq = data_rfm_optimum.Segmentation.value_counts()\n",
    "# Concat both dataframe\n",
    "data_rfm_optimum_groupby = pd.concat([data_rfm_optimum_groupby_median, data_rfm_optimum_groupby_freq], axis=1)\n",
    "data_rfm_optimum_groupby\n",
    "\n",
    "# Group dataframe by segmentation\n",
    "# data_rfm_optimum_groupby = data_rfm_optimum.groupby('Segmentation').mean()\n",
    "# data_rfm_optimum_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_k = 6\n",
    "data_rfm_optimum_groupby['R_Score'] = ((data_rfm_optimum_groupby['Recency'].rank(ascending=False))*5/chosen_k).round().astype(int)\n",
    "data_rfm_optimum_groupby['F_Score'] = ((data_rfm_optimum_groupby['Frequency'].rank(ascending=True))*5/chosen_k).round().astype(int)\n",
    "data_rfm_optimum_groupby['M_Score'] = ((data_rfm_optimum_groupby['Monetary'].rank(ascending=True))*5/chosen_k).round().astype(int)\n",
    "data_rfm_optimum_groupby.sort_values(['Recency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rfm_optimum_groupby['RFM_score'] = data_rfm_optimum_groupby.R_Score.map(str)+data_rfm_optimum_groupby.F_Score.map(str)+data_rfm_optimum_groupby.M_Score.map(str)\n",
    "# data_rfm_optimum_groupby = data_rfm_optimum_groupby.drop(columns=['R_Score','F_Score','M_Score'])\n",
    "data_rfm_optimum_groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RFM score has been created!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F4. Define Customer Segment Name & Insight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut pendefinisian RFM Score into Customer Segmentation & its Actionable insight **BASED ON R-F-M VALUE INTO RFM SCORE**. Source: **CleverTap & Prospectsoft** --> CleverTap: Well known organization (specialising in CRM & eCommerce), SaaS based customer lifecycle management and mobile marketing company which provides mobile app analytics and user engagement products to 4,000 clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping for correlate RFM Score with real customer segmentation name\n",
    "seg_map = {\n",
    "    r'11[1-5]': 'Lost',\n",
    "    r'[1-2][1-2][1-5]': 'Hibernating',\n",
    "    r'[1-2][3-4][1-5]': 'At Risk',\n",
    "    r'[1-2]5[1-5]': 'Cant Loose Them',\n",
    "    r'3[1-2][1-5]': 'About to Sleep',\n",
    "    r'33[1-5]': 'Need Attention',\n",
    "    r'[3-4][4-5][1-5]': 'Loyal Customers',\n",
    "    r'41[1-5]': 'Promising',\n",
    "    r'51[1-5]': 'New Customers',\n",
    "    r'[4-5][2-3][1-5]': 'Potential Loyalists',\n",
    "    r'5[4-5][1-5]': 'Champions'\n",
    "}\n",
    "\n",
    "# Create new columns for customer segmentation based on seg_map\n",
    "data_rfm_optimum_groupby['RFM_name_validation'] = data_rfm_optimum_groupby.RFM_score.replace(seg_map, regex=True)\n",
    "\n",
    "# copy dataframe for treemap\n",
    "df_treemap = data_rfm_optimum_groupby.copy()\n",
    "data_rfm_optimum_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create for loop for df into RFM Result + actionable insight\n",
    "\n",
    "def add_segment_action(df, column_name):\n",
    "    j = range(len(df))\n",
    "    segment_desc = []\n",
    "    action_desc = []\n",
    "\n",
    "    for i in j:\n",
    "        if df[column_name][i] == 'Champions':\n",
    "            segment_desc.append('Bought recently, buy often, & spend the most')\n",
    "            action_desc.append('Reward them, likely to be early adopters of a new product, they will promote your brand')\n",
    "        elif df[column_name][i] == 'Loyal Customers':\n",
    "            segment_desc.append('Spend good money with us often & are responsive to promotions')\n",
    "            action_desc.append('Upsell higher value products, ask for reviews, engage them')\n",
    "        elif df[column_name][i] == 'Potential Loyalists':\n",
    "            segment_desc.append('Recent customer but spending a good amount & have bought more than once')\n",
    "            action_desc.append('Offer membership or loyalty programs, recommend other products to them')\n",
    "        elif df[column_name][i] == 'New Customers':\n",
    "            segment_desc.append('Bought more recently, but dont purchase often')\n",
    "            action_desc.append('provide onboard support, give them early success, and start building a relationship')\n",
    "        elif df[column_name][i] == 'Promising':\n",
    "            segment_desc.append('Recent shoppers but havent spent much')\n",
    "            action_desc.append('create brand awareness, offer a fee product sample')\n",
    "        elif df[column_name][i] == 'Need Attention':\n",
    "            segment_desc.append('Above average recently, frequency, & monetary values - will lose them of not reactivated')\n",
    "            action_desc.append('make limited-time offers, recommend a product to them based on past purchases, reactivate them')\n",
    "        elif df[column_name][i] == 'About to Sleep':\n",
    "            segment_desc.append('Below average recently, frequency & monetary values - will lose them of not reactivated')\n",
    "            action_desc.append('Share valuable resources, recommend a popular product at discount, and reconnect with them')\n",
    "        elif df[column_name][i] == 'At Risk':\n",
    "            segment_desc.append('Spent big money and purchased often but havent purchased for a long time')\n",
    "            action_desc.append('Send personalized emails to reconnect, offer discounts, and provide a helpful resource')\n",
    "        elif df[column_name][i] == 'Cant Loose Them':\n",
    "            segment_desc.append('Made biggest purchases frequently but havent purchased for a long time')\n",
    "            action_desc.append('Win the back with a newer product, dont lose them to competition, talk to them')\n",
    "        elif df[column_name][i] == 'Hibernating':\n",
    "            segment_desc.append('Last purchase was long back - these are low spenders who have placed few orders')\n",
    "            action_desc.append('Offer other relevant products and special discounts, recreate brand value')        \n",
    "        else:\n",
    "            segment_desc.append('Lowest recency, frequency, & monetary scores')\n",
    "            action_desc.append('Revive interest with reach out campaign, ignore them')   \n",
    "\n",
    "    segment_desc_pd = pd.DataFrame(segment_desc, columns=['RFM_desc'])\n",
    "    action_desc_pd = pd.DataFrame(action_desc, columns=['RFM_action'])\n",
    "\n",
    "    # Concate between Inference-Set and Rating's Prediction\n",
    "    df = pd.concat([df, segment_desc_pd, action_desc_pd], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concate between Inference-Set and Rating's Prediction\n",
    "data_rfm_optimum_groupby_complete = add_segment_action(data_rfm_optimum_groupby, 'RFM_name_validation')\n",
    "data_rfm_optimum_groupby_complete.drop(columns=['R_Score','F_Score','M_Score','RFM_score'],inplace=True)\n",
    "data_rfm_optimum_groupby_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Segmentation based on group of customer has been created!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F5. Create Complete Customer Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the customer segmentation name, segmentation description, and its actionable has been all defined by segmentaion group in the previous section. Now its time to correlate the result with Cust ID dataset, so you can look up Cust ID by their segmentation and all the RFM detail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Adding Customer ID with RFM optimum\n",
    "data_rfm_optimum = pd.concat([data_rfm_raw['Customer ID'], data_rfm_optimum], axis=1)\n",
    "# Show concat result\n",
    "data_rfm_optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create for loop for data_rfm_cluster into RFM Result\n",
    "def add_segment_name(df, column_name):\n",
    "    m = range(len(df))\n",
    "\n",
    "    name_desc = []\n",
    "\n",
    "    for i in m:\n",
    "        if df[column_name][i] == 0:\n",
    "            name_desc.append('About to Sleep')\n",
    "        elif df[column_name][i] == 1:\n",
    "            name_desc.append('Hibernating')\n",
    "        elif df[column_name][i] == 2:\n",
    "            name_desc.append('Champions')\n",
    "        elif df[column_name][i] == 3:\n",
    "            name_desc.append('Lost')\n",
    "        elif df[column_name][i] == 4:\n",
    "            name_desc.append('At Risk')\n",
    "        else:\n",
    "            name_desc.append('Potential Loyalists')\n",
    "\n",
    "    name_desc_pd = pd.DataFrame(name_desc, columns=['RFM_name'])\n",
    "\n",
    "    # Concate between Inference-Set and Rating's Prediction\n",
    "    df = pd.concat([df, name_desc_pd], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concate between Inference-Set and Rating's Prediction\n",
    "data_rfm_with_segment = add_segment_name(data_rfm_optimum, 'Segmentation')\n",
    "data_rfm_with_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rfm_complete = add_segment_action(data_rfm_with_segment, 'RFM_name')\n",
    "data_rfm_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Segmentaion based on Customer ID has been created!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G. Segmenting Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Percentage\n",
    "df_treemap['Percentage'] = round(100 * df_treemap[\"Segmentation\"] / sum(df_treemap[\"Segmentation\"]), 2)\n",
    "# df_treemap.drop(columns=['R_Score','F_Score','M_Score','RFM_score'],inplace=True)\n",
    "df_treemap[\"Label\"] = df_treemap[\"RFM_name_validation\"] + \" (\" + df_treemap[\"Percentage\"].astype(\"str\") + \"%)\"\n",
    "df_treemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Chart or Plot\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x=df_treemap.RFM_name_validation, y=df_treemap.Segmentation, data=df_treemap)\n",
    "plt.title(\"Cluster of Customer Segmentation (Chart 9)\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cluster of Customer Segmentation chart shows the number of customers for each segmentation. The highest number of customers is in the Hibernating segment (Last purchase was long back - these are low spenders who have placed few orders), then in second place is Lost Customer (Lowest recency, frequency, & monetary scores). Based on the level of risk & mitigation, Hibernating and Lost customers are the segmentation with the highest risk for loss of potential sales (bad news, they are most likely to stop buying the product from the shop). On the other hand, for the Champions segmentation (Bought recently, buy often, & spend the most) and Potential Loyalist (Recent customer but spending a good amount & have bought more than once), they help online stores in promoting the products they sell and help increase their sales. indirect store sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Chart or Plot\n",
    "plt.subplots(figsize=(20,10))\n",
    "squarify.plot(sizes=df_treemap[\"Segmentation\"], label=df_treemap[\"Label\"], color=sns.color_palette(\"Spectral\"))\n",
    "plt.title(\"Cluster of Customer Segmentation - Treemap style (Chart 10)\")\n",
    "plt.axis('off')\n",
    "fig.set_facecolor('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Konversi visualisasi Cluster of Customer Segmentation into treemap done!** Lets proceed with more evaluasion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Create Chart or Plot\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x='RFM_name', y='Recency', data=data_rfm_complete)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Recency - The Lowest The Better (Chart R)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.barplot(x='RFM_name', y='Frequency', data=data_rfm_complete)\n",
    "plt.title('Frequency - The Highest The Better (Chart F)')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.barplot(x='RFM_name', y='Monetary', data=data_rfm_complete)\n",
    "plt.title('Monetary - The Highest The Better (Chart M)')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# set the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To re-assure, whether the segmentation has performed well or has represented the entire data, it can be seen that the differences between the ON R-F-M based clusters are clear, or the differences can be seen clearly and according to the characters of RFM. For example, customer Champions segmentation based on recency (see Chart R) seems to have the lowest value which is true. And in terms of Freq and Monetary, Champions have the highest value among the other segments, which is visible on Charts F and M. This evaluation logic also applies to other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Create Chart or Plot\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "sns.lineplot(x='Recency', y='Frequency', hue='RFM_name', data=data_rfm_complete)\n",
    "plt.title('Recency vs Frequency (Chart RF)')\n",
    "plt.ylim(0, 1000)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "sns.lineplot(x='Recency', y='Monetary', hue='RFM_name', data=data_rfm_complete)\n",
    "plt.title('Recency vs Monetary (Chart RM)')\n",
    "plt.ylim(0, 25000)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "sns.lineplot(x='Frequency', y='Monetary', hue='RFM_name', data=data_rfm_complete)\n",
    "plt.title('Frequency vs Monetary (Chart FM')\n",
    "plt.ylim(0, 150000)\n",
    "\n",
    "# set the spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the chart above, it is clearer to see the segmentation divided by each other. For example, it is clearer that there is a GAP recency of the 0 recency value on Lost Customer (see RF Chart). For At Risk customers, it can be seen that the frequency is quite explosive on several days as long as the dataset (RF Chart) has a large monetary value (RM Chart), but the recency is quite high. So the store can be sure to lose if the online store doesn't do anything to them.\n",
    "\n",
    "`“80% of your sales come from 20% of your customers.\"` Pareto Principle. **you're in business largely because of the support of a fraction of your customer base: your best customers.** Based on this principle, So the priority of the online store in this case is to immediately approach customers more, whether they are already best customers (Champions & Loyal) or the other way around (At Risk, Hibernate or Lost Customer). What should be done?\n",
    "\n",
    "Here are some strategies and actionable insights that this online store can do.\n",
    "\n",
    "**Champions** --> Reward them, likely to be early adopters of a new product, they will promote your brand.\n",
    "\n",
    "**Potential Loyalists** --> Offer membership or loyalty programs, recommend other products to them.\n",
    "\n",
    "**About to Sleep** --> Share valuable resources, recommend a popular product at discount, and reconnect with them.\n",
    "\n",
    "**At Risk** --> Send personalized emails to reconnect, offer discounts, and provide a helpful resource.\n",
    "\n",
    "**Hibernating** --> Offer other relevant products and special discounts to recreate brand value.\n",
    "\n",
    "**Lost Customer** --> Revive interest with reach out campaign, ignore them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H. Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Files\n",
    "with open('model_kmeans_pipe.pkl', 'wb') as file_1:\n",
    "  pickle.dump(model_kmeans_pipe, file_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming data inf into log\n",
    "data_rfm_inf_log = np.log(data_rfm_inf+1)\n",
    "# predict segmentation of dataset\n",
    "data_rfm_inf_segment = model_kmeans_pipe.predict(data_rfm_inf_log)\n",
    "# create dataframe of prediction result\n",
    "df_data_rfm_inf_segment = pd.DataFrame(data_rfm_inf_segment,columns=['Segmentation'])\n",
    "# concate between inference and clustering result\n",
    "df_final_inf = pd.concat([data_rfm_inf,df_data_rfm_inf_segment],axis=1)\n",
    "# add segment name and actionable insight\n",
    "df_final_inf = add_segment_name(df_final_inf, 'Segmentation')\n",
    "df_final_inf = add_segment_action(df_final_inf, 'RFM_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer segmentaion with Inference data has been created just fine and complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# J. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFM & Clustering Model Conclusion\n",
    "\n",
    "Membuat Segmentasi Customer menggunakan RFM analysis dengan tools Machine Learning K-Means berhasil dilakukan dengan hasil clustering yang clear dan impactful. Machine Learning K-Means pada dasarnya digunakan untuk memudahkan & mempercepat proses calculasi segmentasi (automasi machine learning), serta memungkinkan seorang data science untuk bekerja dengan lebih dari 1 juta data (yang tidak bisa dilakukan menggunakan excel). Selain itu, segmentasi yang dihasilkan lebih proper secara statistik, karena machine learning melihat internal similirity feature RFM secara optimal (pattern dari features) yang tidak bisa dilakukan hanya menggunakan Excel based biasa (yang hanya sekadar menggunakan teknik quartil pada saat konversi nilai RFM value into score). Adapun pada case ini, secara umum distribusi data after dimensi direduksi menjadi 2D (after PCA has been done), terlihat bahwa kerapatan dari dataset cukup padat dan merata, oleh karena itu lah KMeans digunakan untuk tetap dapat membagi dataset yang rapat dan merata tersebut ke dalam beberapa cluster. Jika algoritma lain seperti DBScan dan Spectral digunakan, maka hasil clustering tidak akan sesuai dengan konsep dari RFM itu sendiri (which is micro segmentation). Atas beberapa advantages tersebut K Means juga memiliki disadvantages, yaitu kurang performnya model jika dataset yang digunakan skewed dan standar deviasi yang kurang baik. Untuk mengatasi hal tersebut, log transformation pada dataset RFM perlu dilakukan sehingga penentuan K terbaik untuk RFM dapat lebih mudah untuk ditentukan (menggunakan PCA scatterplot & snakeplot). Dari hasil simulasi K, maka diperoleh nilai K terbaik, yaitu 6. Hasil evaluasi menunjukkan bahwa hasil clustering clear and very well devided between customer segmentation. Ke enam segmentasi tersebut adalah **Champions**, **Potential Loyalists**, **About to Sleep**, **At Risk**, **Hibernating**, serta **Lost Customer** + additional detail dan actionable insight."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
